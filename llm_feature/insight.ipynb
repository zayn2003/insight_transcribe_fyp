{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn-husyn/project/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch, whisper, re, difflib\n",
    "\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset.csv\", encoding=\"utf-8\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "insight_dict = {\n",
    "    str(word).strip().lower(): str(expl).strip()\n",
    "    for word, expl in zip(df[\"word\"], df[\"explanation\"])\n",
    "}\n",
    "\n",
    "cs_keywords = list(insight_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_path = \"./qwen_lora_finetuned\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_path)\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_path, trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(base, peft_path, torch_dtype=torch.float32)\n",
    "model = model.merge_and_unload().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_cs_terms(text):\n",
    "    return list({term for term in cs_keywords if re.search(rf'\\b{re.escape(term)}\\b', text.lower())})\n",
    "\n",
    "def lookup_or_generate(term):\n",
    "    key = term.lower()\n",
    "    if key in insight_dict:\n",
    "        return insight_dict[key]\n",
    "    m = difflib.get_close_matches(key, insight_dict.keys(), n=1, cutoff=0.85)\n",
    "    if m:\n",
    "        return insight_dict[m[0]]\n",
    "    prompt = f\"Explain this computer-science term in simple words:\\n\\nTerm: {term}\\nExplanation:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=80, temperature=0.7,\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    expl = text.split(\"Explanation:\")[-1].strip().split(\"\\n\")[0]\n",
    "    insight_dict[key] = expl \n",
    "    return expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_insights(audio_path):\n",
    "    res = whisper_model.transcribe(audio_path)\n",
    "    transcript = res[\"text\"]\n",
    "    terms = extract_cs_terms(transcript)\n",
    "    if not terms:\n",
    "        return transcript, [], [\"No CS jargon found.\"]\n",
    "    insights = [f\" {t}: {lookup_or_generate(t)}\" for t in terms]\n",
    "    return transcript, terms, insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayn-husyn/project/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  Hello world, I'm Karyan Philbin and welcome to Crash Course Computer Science. So, computers really have allowed us to do some pretty amazing things. Think global telecommunications, international commerce, global transportation, breakthroughs in medicine, distributed education, online shopping, online dating, and just the internet in general. Computers are allowing us to explore our own world and other worlds, and of course some seemingly mundane things like permitting us to smile on our pets from work, or communicating with our friends in a nearly indecisive or voice stream of emoji. But don't call computers magical, they are not. I repeat, are not magical. So before we get into what we're going to talk about in this series, it might be useful to tell you what we are not going to talk about. We aren't going to teach you how to program. Programming is a really crucial aspect of computer science, and we will get to the rules that guide the logic of hardware and software design. But we aren't going to teach you how to program an Arduino to water your plan, or how to change the CSS on your grandma's sewing blog, so visitors' curses turn into kittens. This also isn't a computing course, or at least how computing is thought of in the US. Computing here is a goal, it's what computers do, and we'll talk about some of that for sure, but our goal for this course is much broader. But computing means other things in other countries. It's all pretty confusing, but what we are going to look at are the history of computers, even before we had electricity. We're going to retrace the design decisions that have given us our present-day components. We're going to talk about how operating systems work or don't work. How the YouTubes get to you over the internet. How our smartphones and other smart devices are, well, getting smarter. And of course, mysterious futuristic stuff, like quantum computing and frustrating present-day stuff like hacking. It's a lot to cover, but I suppose before we get started, I should introduce myself. I'm Carrie Ann Philbin. Hello, I'm an award-winning computing teacher, author of Adventures in Raspberry Pi, and the creator of a YouTube video series for teenagers called the Geekgold Diaries, which includes stuff like interviews with women working in technology, computer science-based tutorials, and hands-on digital-maker-style projects. In my day job, I help people learn about technology, and how to make things with computers as director of education for the Raspberry Pi Foundation, which is a charity based in Cambridge in the UK. Needless to say, I am passionate about this stuff. But not because computers are these amazing devices that are always making our lives easier, sometimes that's debatable, but because computers in our group have become pivotal in our society. From our cars and thermostats to pacemakers and cell phones, computers are everywhere, and it's my hope that by the end of this course, you'll have a better understanding and appreciation for how far we've come and how far they may take us. I'll see you next week.\n",
      "Detected jargon: ['quantum computing']\n",
      " quantum computing: A computing paradigm leveraging quantum mechanics to solve complex problems faster than classical computers.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    path = \"Crash Course Computer Science Preview.mp3\"\n",
    "    txt, jargon, outs = audio_to_insights(path)\n",
    "    print(\"Transcript:\", txt)\n",
    "    print(\"Detected jargon:\", jargon)\n",
    "    print(\"\\n\".join(outs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project)",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
